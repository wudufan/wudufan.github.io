<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Research</title>
  <meta name="description" content="Research fellow at MGH & HMS on CT imaging.
">
  <link rel="shortcut icon" type="image/png" href="/images/icon.png">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="/research/">
  <link rel="alternate" type="application/rss+xml" title="Dufan Wu" href="/feed.xml" />
<link rel='stylesheet' id='open-sans-css'  href='//fonts.googleapis.com/css?family=Open+Sans%3A300italic%2C400italic%2C600italic%2C300%2C400%2C600&#038;subset=latin%2Clatin-ext&#038;ver=4.2.4' type='text/css' media='all' />
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:600italic,600,400,400italic' rel='stylesheet' type='text/css'>
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Dufan Wu</a>


    <nav class="site-nav">

      <a href="#" class="menu-icon menu.open">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>  

    <div class="trigger"><h1>Navigation</h1>

 		<ul class="menu">

    
    
     <li><a href="/" class="page-link">About</a>
    
    </li>
    
    
     <li><a href="/research/" class="page-link">Research</a>
    
    </li>
    
    
     <li><a href="/publications/" class="page-link">Publications</a>
    
    </li>
    
</ul>


     </div>  
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Research</h1>
  </header>

  <article class="post-content">
    <p>Here is a non-exhaustive list of my research projects.</p>

<details>
<summary><b>Unsupervised learning for medical image denoising without high-quality images</b></summary><br />
Currently most neural network-based medical image denoising methods require matched or unmatched high-quality images as reference during training, which are inaccessible under certain circumstances such as dynamic imaging. Based on the recently proposed Noise2noise method, we proposed to train denoising network from noisy measurement only by splitting the measurement into two sets and mapping the reconstructed images from one set to the other. Theoretical analysis was done which indicated the feasible conditions of the Noise2noise scheme. We further proposed a novel consensus loss function based on the theory to better utilize both noisy measurements.<br />
<a href="https://arxiv.org/abs/1906.03639" target="_blank"><div class="color-button">arXiv</div></a>



</details>
<p><br /></p>

<details>
<summary><b>Iterative Low-dose CT reconstruction with unsupervised manifold learning via k-sparse autoencoder</b></summary><br />
A k-sparse autoencoder was built to learn the manifold of normal-dose CT images in an unsupervised manner. Then objective function was built concerning both data fidelity and distance to the manifold. The objective function was solved via alternative optimization of the data fidelity part and the manifold part, which was a monotonic algorithm given appropriate step size. K-sparse autoencoder was built on patches and trained greedily by keeping only the k largest encodes at each training iteration. Furthermore, the k-sparse autoencoder demonstrated superior edge preservation performance compared to normal autoencoders and L1-sparse autoencoder.<br />
<a href="https://ieeexplore.ieee.org/abstract/document/8038851" target="_blank"><div class="color-button">paper</div></a>

</details>
<p><br /></p>

<details>
<summary><b>Undersampled image reconstruction with computational efficient neural network</b></summary><br />
One way to do image reconstruction with deep neural networks is unrolling iterative algorithms to finite steps and parameterize prior-related terms with trainable neural networks. By taking the measurement into consideration, the unroll approach achieved promising performance in undersampled problems such as few-view, limited-angle and low-dose. However, it requires significant amount of memory and training time for 3D volume reconstruction which is not practical for most current hardware. We proposed a computationally efficient training approach which broke the learning into sequential image-domain trainings and significantly reduced the computational cost. The estimated memory cost was reduced from 417 GB to 2 GB for a CT reconstruction problem with ordinal-size image (640*640*128). It was also demonstrated the proposed method could achieve similar performance to unrolled network.<br />
<a href="https://aapm.onlinelibrary.wiley.com/doi/abs/10.1002/mp.13627" target="_blank"><div class="color-button">paper</div></a>
<a href="https://arxiv.org/abs/1810.03999" target="_blank"><div class="color-button">arXiv</div></a>

</details>
<p><br /></p>

<details>
<summary><b>Low-dose dual energy CT reconstruction via deep neural networks</b></summary><br />
Dual energy CT (DECT) is generally not feasible for very large patients due to the heavily attenuated photon flux under 80kVp. This project investigated the deep neural network-based image reconstruction for low-flux scenarios of DECT. Normal-dose and half-dose DECT scans were acquired for each of 40 patients under IRB approval. Noise was inserted to the normal-dose raw data to simulate low-dose scans and a learned primal-dual network was trained with L2 norm on 30 patients. The network was tested on the rest 10 patientsâ€™ real half-dose scans and achieved significantly reduced noise without bias.<br />
<a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11072/1107206/Learned-primal-dual-reconstruction-for-dual-energy-computed-tomography-with/10.1117/12.2534943.short?SSO=1" target="_blank"><div class="color-button">paper</div></a>

</details>
<p><br /></p>

<details>
<summary><b>Task-based image reconstruction for lung nodule detection</b></summary><br />
Current image reconstruction is optimized for visual quality of the images to radiologist. However, computer aided diagnosis system could rely on very different features for automatic lesion detection than human observers. We built a lung nodule detection network whose input was the sinogram rather than the images. The deep neural network consisted of a learned primal-dual reconstruction network and 3D CNN detection network. Improved FROC performance was achieved with the task-based image reconstruction on simulated sparse-view sinograms compared to reconstruction optimized for L2 norm loss on images.<br />
<a href="https://link.springer.com/chapter/10.1007/978-3-030-00919-9_5" target="_blank"><div class="color-button">paper</div></a>    
<a href="https://arxiv.org/abs/1711.02074" target="_blank"><div class="color-button">arXiv</div></a>

</details>
<p><br /></p>


  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

<!--     <h2 class="footer-heading">Dufan Wu</h2> -->

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li><strong>Dufan Wu</strong></li>
          <li><a href="mailto:dwu6@mgh.harvard.edu">dwu6@mgh.harvard.edu</a></li>
          <li><a href="mailto:wudufan33@gmail.com">wudufan33@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>
            <a href="https://github.com/wudufan">
              <span class="username">wudufan</span>
            </a>
          </li>
          

          

          
          <li>
              <span class="icon  icon--linkedin">
                <svg viewBox="0 50 512 512" >
                  <path fill="#828282" d="M150.65,100.682c0,27.992-22.508,50.683-50.273,50.683c-27.765,0-50.273-22.691-50.273-50.683
                  C50.104,72.691,72.612,50,100.377,50C128.143,50,150.65,72.691,150.65,100.682z M143.294,187.333H58.277V462h85.017V187.333z
                  M279.195,187.333h-81.541V462h81.541c0,0,0-101.877,0-144.181c0-38.624,17.779-61.615,51.807-61.615
                  c31.268,0,46.289,22.071,46.289,61.615c0,39.545,0,144.181,0,144.181h84.605c0,0,0-100.344,0-173.915
                  s-41.689-109.131-99.934-109.131s-82.768,45.369-82.768,45.369V187.333z"/>
                </svg>
              </span>
            <a href="https://www.linkedin.com/in/wudufan">
              <span class="username">wudufan</span>
            </a>
          </li>
          


          
          </svg>
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
         <p class="text">
Research fellow at MGH & HMS on CT imaging.
 
      </div>
    </div>

  </div>

</footer>

  </body>

</html>